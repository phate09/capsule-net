{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CodeCell': {'cm_config': {'autoCloseBrackets': False}},\n",
       " 'Cell': {'cm_config': {'lineNumbers': True}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plnn.mnist_sequential import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain(input_tensor, eps_size):\n",
    "    return torch.stack((input_tensor - eps_size, input_tensor + eps_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('save/mnist_sequential_cnn.pt'))\n",
    "model.cuda()\n",
    "dataset = MNIST('./data', train=True, download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])),  # load the testing dataset\n",
    "batch_size=10\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=1)#retrieve items 1 at a time\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationNetwork(nn.Module):\n",
    "    def __init__(self, base_network,true_class_index):\n",
    "        super(VerificationNetwork, self).__init__()\n",
    "        self.true_class_index = true_class_index\n",
    "#         self.property_layers=[]\n",
    "#         n_classes = base_network.layers[-1].out_features\n",
    "#         print(f'n_classes={n_classes}')\n",
    "#         for true_class_index in range(n_classes):\n",
    "#             self.property_layers.append( self.attach_property_layers(base_network,true_class_index))\n",
    "        self.property_layer = self.attach_property_layers(base_network,self.true_class_index)\n",
    "        self.layers=base_network.layers+[self.property_layer]\n",
    "        self.out=nn.Sequential(*self.layers)\n",
    "\n",
    "    '''need to  repeat this method for each class so that it describes the distance between the corresponding class \n",
    "    and the closest other class'''\n",
    "    def attach_property_layers(self, model: Net,true_class_index:int):\n",
    "        n_classes = model.layers[-1].out_features\n",
    "        cases = []\n",
    "        for i in range(n_classes):\n",
    "            if i == true_class_index:\n",
    "                continue\n",
    "            case = [0] * n_classes  # list of zeroes\n",
    "            case[true_class_index] = 1  # sets the property to 1\n",
    "            case[i] = -1\n",
    "            cases.append(case)\n",
    "        weights = np.array(cases)\n",
    "#         print(f'weight={weights}')\n",
    "        print(f'weights.size()={weights.shape}')\n",
    "        weightTensor = nn.Linear(in_features=n_classes, out_features=n_classes-1,\n",
    "                                 bias=False)\n",
    "        print(f'initial weightTensor size={weightTensor.weight.size()}')\n",
    "        weightTensor.weight.data = torch.from_numpy(weights).float()\n",
    "        print(f'final weightTensor size={weightTensor.weight.size()}')\n",
    "        return weightTensor\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.base_network(x)\n",
    "        x = self.out(x)\n",
    "        print(x)\n",
    "        print(x.size())\n",
    "        return torch.min(x,dim=1,keepdim=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:torch.Size([10, 1, 28, 28])\n",
      "domain size:torch.Size([2, 10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#get the data and label\n",
    "data, target =next(iter(test_loader))\n",
    "print(f'data size:{data.size()}')\n",
    "# print(data[0])\n",
    "# create the domain\n",
    "domain_raw = generate_domain(data,0.001)\n",
    "data_size=data.size()\n",
    "print(f'domain size:{domain_raw.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "True class=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n",
      "weights.size()=(9, 10)\n",
      "initial weightTensor size=torch.Size([9, 10])\n",
      "final weightTensor size=torch.Size([9, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VerificationNetwork(\n",
       "  (property_layer): Linear(in_features=10, out_features=9, bias=False)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=9, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[-1].out_features)\n",
    "print(f'True class={target}')\n",
    "single_true_class=7\n",
    "verification_model=VerificationNetwork(model,single_true_class)\n",
    "verification_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 11.0055,  15.3589,   8.2809,   6.8677,  14.8731,  10.9445,  21.4909,\n",
      "          10.0609,   9.6480],\n",
      "        [ -7.7150, -10.4823, -18.2218, -11.7193,   2.1209,  -9.6300,  -8.6503,\n",
      "         -10.0324,   2.7475],\n",
      "        [  5.1997,  -6.2795,  -0.4463,   1.3481,   1.1025,   1.9049,   1.9214,\n",
      "          -0.2725,   3.0070],\n",
      "        [ -9.4011,   2.2561,  -0.4023,   2.9498,   3.5961,   0.2800,  -0.8867,\n",
      "           6.4764,   1.0052],\n",
      "        [  2.4483,   4.7913,   1.2583,   4.6639,  -7.3499,   2.8749,   1.4881,\n",
      "           1.9828,  -2.7216],\n",
      "        [  7.9259,  -5.7664,   3.0979,   4.1033,   3.8198,   7.1183,   6.8119,\n",
      "           2.6324,   5.1645],\n",
      "        [  5.9190,   4.3122,   5.9192,   3.6975,  -8.7227,  -0.4677,   3.7314,\n",
      "          -3.4629,  -1.3713],\n",
      "        [  7.7114,   3.7077,   0.4748,  -0.4291,  -0.7820,   1.3426,   6.6008,\n",
      "           2.8977,  -8.0057],\n",
      "        [ -2.7740,  -3.7870,  -6.4784,  -0.0640,  -5.3700,  -9.8601,  -9.7806,\n",
      "          -5.8107,  -5.3476],\n",
      "        [  6.3015,  12.6781,   8.3272,   3.4601,  -1.2599,   5.7796,  12.4307,\n",
      "           1.2595,  -6.4999]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "torch.Size([10, 9])\n",
      "test_out=tensor([[  6.8677],\n",
      "        [-18.2218],\n",
      "        [ -6.2795],\n",
      "        [ -9.4011],\n",
      "        [ -7.3499],\n",
      "        [ -5.7664],\n",
      "        [ -8.7227],\n",
      "        [ -8.0057],\n",
      "        [ -9.8601],\n",
      "        [ -6.4999]], device='cuda:0', grad_fn=<MinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_out=verification_model(data.cuda().view(-1,784))\n",
    "print(f'test_out={test_out}')\n",
    "# print(f'test_out[0]={test_out[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "test_out=model(data.cuda().view(-1,784))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 784])\n"
     ]
    }
   ],
   "source": [
    "domain=domain_raw.view(2,batch_size,-1)\n",
    "print(domain.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_ub_point, global_ub = net.get_upper_bound(domain)\n",
    "# global_lb = net.get_lower_bound(domain)\n",
    "\n",
    "\n",
    "def get_upper_bound(domain,model):\n",
    "    #we try get_upper_bound\n",
    "    nb_samples = 1024\n",
    "    nb_inp = domain.size()[2:]  #get last dimensions\n",
    "    print(nb_inp)\n",
    "    # Not a great way of sampling but this will be good enough\n",
    "    # We want to get rows that are >= 0\n",
    "    rand_samples_size = [batch_size, nb_samples] + list(nb_inp)\n",
    "    print(rand_samples_size)\n",
    "    rand_samples = torch.zeros(rand_samples_size)\n",
    "    print(rand_samples.size())\n",
    "    # print(rand_samples)\n",
    "    rand_samples.uniform_(0, 1)\n",
    "    # print(rand_samples)\n",
    "    print(rand_samples.size())\n",
    "    domain_lb = domain.select(0, 0).contiguous()\n",
    "    domain_ub = domain.select(0, 1).contiguous()\n",
    "    # print(domain_lb)\n",
    "    print(domain_lb.size())\n",
    "    # print(domain_ub)\n",
    "    print(domain_ub.size())\n",
    "    domain_width = domain_ub - domain_lb\n",
    "    print(domain_width.size())\n",
    "    print(domain_lb.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    print(domain_width.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    domain_lb = domain_lb.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the initial point for the number of examples\n",
    "    print(domain_lb.size())\n",
    "    domain_width = domain_width.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the width for the number of examples\n",
    "    print(domain_width.size())\n",
    "    #those should be the same\n",
    "    print(domain_width.size())\n",
    "    print(rand_samples.size())\n",
    "    inps = domain_lb + domain_width * rand_samples\n",
    "    # print(inps) #each row shuld be different\n",
    "    print(inps.size())\n",
    "    #now flatten the first dimension into the second\n",
    "    flattened_size = [inps.size(0) * inps.size(1)] + list(inps.size()[2:])\n",
    "    print(flattened_size)\n",
    "    #rearrange the tensor so that is consumable by the model\n",
    "    print(data_size)\n",
    "    examples_data_size = [flattened_size[0]] + list(data_size[1:])  #the expected dimension of the example tensor\n",
    "    print(examples_data_size)\n",
    "    var_inps = torch.Tensor(inps).view(examples_data_size)\n",
    "    print(f'var_inps.size()={var_inps.size()}')  #should match data_size\n",
    "    print(inps.size())\n",
    "    # print(inps[0][0])\n",
    "    # print(inps[0][1])\n",
    "    # print(var_inps[0][0])\n",
    "    # print(var_inps[1][0])\n",
    "    outs = model.forward(var_inps.cuda())  #gets the input for the values\n",
    "    print(outs.size())\n",
    "    print(outs[0])  #those two should be very similar but different because they belong to two different random examples\n",
    "    print(outs[1])\n",
    "    print(target.unsqueeze(1))\n",
    "    target_expanded = target.unsqueeze(1).expand(\n",
    "        [batch_size, nb_samples])  #generates nb_samples copies of the target vector, all rows should be the same\n",
    "    print(target_expanded.size())\n",
    "    print(target_expanded)\n",
    "    target_idxs = target_expanded.contiguous().view(\n",
    "        batch_size * nb_samples)  #contains a list of indices that tells which columns out of the 10 classes to pick\n",
    "    print(target_idxs.size())  #the first dimension should match\n",
    "    print(outs.size())\n",
    "    print(outs[target_idxs[0]].size())\n",
    "    outs_true_class = outs.gather(1, target_idxs.cuda().view(-1,\n",
    "                                                             1))  #we choose dimension 1 because it's the one we want to reduce\n",
    "    print(outs_true_class.size())\n",
    "    # print(outs[0])\n",
    "    # print(target_idxs[1])\n",
    "    # print(outs[1][0])#these two should be similar but different because they belong to different examples\n",
    "    # print(outs[0][0])\n",
    "    print(outs_true_class.size())\n",
    "    outs_true_class_resized = outs_true_class.view(batch_size, nb_samples)\n",
    "    print(outs_true_class_resized.size())  #resize outputs so that they each row is a different element of each batch\n",
    "    upper_bound, idx = torch.min(outs_true_class_resized,\n",
    "                                 dim=1)  #this returns the distance of the network output from the given class, it selects the class which is furthest from the current one\n",
    "    print(upper_bound.size())\n",
    "    print(idx.size())\n",
    "    print(idx)\n",
    "    print(upper_bound)\n",
    "    # rearranged_idx=idx.view(list(inps.size()[0:2]))\n",
    "    # print(rearranged_idx.size()) #rearranged idx contains the indexes of the minimum class for each example, for each element of the batch\n",
    "    print(f'idx size {idx.size()}')\n",
    "    print(f'inps size {inps.size()}')\n",
    "    print(idx[0])\n",
    "    # upper_bound = upper_bound[0]\n",
    "    unsqueezed_idx = idx.cuda().view(-1, 1)\n",
    "    print(f'single size {inps[0][unsqueezed_idx[0][0]][:].size()}')\n",
    "    print(f'single size {inps[1][unsqueezed_idx[1][0]][:].size()}')\n",
    "    print(f'single size {inps[2][unsqueezed_idx[2][0]][:].size()}')\n",
    "    ub_point = [inps[x][idx[x]][:].numpy() for x in range(idx.size()[0])]\n",
    "    ub_point = torch.tensor(ub_point)\n",
    "    print(\n",
    "        ub_point)  #ub_point represents the input that amongst all examples returns the minimum response for the appropriate class\n",
    "    print(ub_point.size())\n",
    "    # print(unsqueezed_idx.size())\n",
    "    # ub_point = torch.gather(inps.cuda(),1,unsqueezed_idx.cuda())#todo for some reason it doesn't want to work\n",
    "    # print(ub_point.size())\n",
    "    return ub_point, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the method\n",
    "get_upper_bound(domain,verification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lower_bound(domain,model):\n",
    "    '''\n",
    "    input_domain: Tensor containing in each row the lower and upper bound\n",
    "                  for the corresponding dimension\n",
    "    '''\n",
    "    #now try to do the lower bound\n",
    "    import gurobipy as grb\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    gurobi_vars = []\n",
    "    # These three are nested lists. Each of their elements will itself be a\n",
    "    # list of the neurons after a layer.\n",
    "\n",
    "    gurobi_model = grb.Model()\n",
    "    gurobi_model.setParam('OutputFlag', False)\n",
    "    gurobi_model.setParam('Threads', 1)\n",
    "\n",
    "    input_domain=domain.select(1,0)#we use a single domain, not ready for parallelisation yet\n",
    "    print(input_domain.size())\n",
    "\n",
    "    ## Do the input layer, which is a special case\n",
    "    inp_lb = []\n",
    "    inp_ub = []\n",
    "    inp_gurobi_vars = []\n",
    "    for dim in range(input_domain.size()[1]):\n",
    "        ub=input_domain[1][dim]#check this value, it can be messed up\n",
    "        lb=input_domain[0][dim]\n",
    "    #     print(f'ub={ub} lb={lb}')\n",
    "        assert ub>lb , \"ub should be greater that lb\"\n",
    "        #     print(f'ub={ub} lb={lb}')\n",
    "        v = gurobi_model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                              vtype=grb.GRB.CONTINUOUS,\n",
    "                              name=f'inp_{dim}')\n",
    "        inp_gurobi_vars.append(v)\n",
    "        inp_lb.append(lb)\n",
    "        inp_ub.append(ub)\n",
    "    gurobi_model.update()\n",
    "\n",
    "    lower_bounds.append(inp_lb)\n",
    "    upper_bounds.append(inp_ub)\n",
    "    gurobi_vars.append(inp_gurobi_vars)\n",
    "\n",
    "    # print(lower_bounds[0][0])\n",
    "    # print(upper_bounds[0][0])\n",
    "\n",
    "    # print(model.layers[0])\n",
    "    # print(range(0))\n",
    "\n",
    "    layer_idx = 1\n",
    "    for layer in model.layers:\n",
    "        print(f'layer_idx={layer_idx}')\n",
    "        # layer = model.layers[0]\n",
    "        new_layer_lb = []\n",
    "        new_layer_ub = []\n",
    "        new_layer_gurobi_vars = []\n",
    "        if type(layer) is nn.Linear:\n",
    "            print(f'Linear')\n",
    "            for neuron_idx in range(layer.weight.size(0)):\n",
    "                if(layer.bias is None):\n",
    "                    ub = 0\n",
    "                    lb = 0\n",
    "                    lin_expr = 0\n",
    "                else:\n",
    "                    ub = layer.bias.data[neuron_idx]\n",
    "                    lb = layer.bias.data[neuron_idx]\n",
    "                    lin_expr = layer.bias.data[neuron_idx].item() #adds the bias to the linear expression\n",
    "            #     print(f'bias_ub={ub} bias_lb={lb}')\n",
    "                \n",
    "                for prev_neuron_idx in range(layer.weight.size(1)):\n",
    "                    coeff = layer.weight.data[neuron_idx, prev_neuron_idx]#picks the weight between the two neurons\n",
    "            #         print(f'coeff={coeff} upper={coeff*upper_bounds[-1][prev_neuron_idx]} lower={coeff*lower_bounds[-1][prev_neuron_idx]}')\n",
    "    #                 assert coeff*lower_bounds[-1][prev_neuron_idx]!=coeff*upper_bounds[-1][prev_neuron_idx], f\"coeff={coeff} upper={coeff*upper_bounds[-1][prev_neuron_idx]} lower={coeff*lower_bounds[-1][prev_neuron_idx]}\"\n",
    "                    if coeff>=0:\n",
    "                        ub = ub+ coeff*upper_bounds[-1][prev_neuron_idx]#multiplies the ub\n",
    "                        lb = lb+ coeff*lower_bounds[-1][prev_neuron_idx]#multiplies the lb\n",
    "                    else: #inverted\n",
    "                        ub = ub+ coeff*lower_bounds[-1][prev_neuron_idx]#multiplies the ub\n",
    "                        lb = lb+ coeff*upper_bounds[-1][prev_neuron_idx]#multiplies the lb\n",
    "            #         print(f'ub={ub} lb={lb}')\n",
    "#                     assert ub!=lb\n",
    "                    lin_expr = lin_expr+ coeff.item() * gurobi_vars[-1][prev_neuron_idx]#multiplies the unknown by the coefficient\n",
    "            #         print(lin_expr)\n",
    "                v = gurobi_model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                                          vtype=grb.GRB.CONTINUOUS,\n",
    "                                          name=f'lay{layer_idx}_{neuron_idx}')\n",
    "                gurobi_model.addConstr(v == lin_expr)\n",
    "                gurobi_model.update()\n",
    "            #     print(f'v={v}')\n",
    "                gurobi_model.setObjective(v, grb.GRB.MINIMIZE)\n",
    "                gurobi_model.optimize()\n",
    "            #          print(f'gurobi status {gurobi_model.status}')\n",
    "                assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "                # We have computed a lower bound\n",
    "                lb = v.X\n",
    "                v.lb = lb\n",
    "\n",
    "                # Let's now compute an upper bound\n",
    "                gurobi_model.setObjective(v, grb.GRB.MAXIMIZE)\n",
    "                gurobi_model.update()\n",
    "                gurobi_model.reset()\n",
    "                gurobi_model.optimize()\n",
    "                assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "                ub = v.X\n",
    "                v.ub = ub\n",
    "\n",
    "                new_layer_lb.append(lb)\n",
    "                new_layer_ub.append(ub)\n",
    "                new_layer_gurobi_vars.append(v)\n",
    "        elif type(layer) == nn.ReLU:\n",
    "            print('Relu')\n",
    "            for neuron_idx, pre_var in enumerate(gurobi_vars[-1]):\n",
    "                pre_lb = lower_bounds[-1][neuron_idx]\n",
    "                pre_ub = upper_bounds[-1][neuron_idx]\n",
    "\n",
    "                v = gurobi_model.addVar(lb=max(0, pre_lb),\n",
    "                                      ub=max(0, pre_ub),\n",
    "                                      obj=0,\n",
    "                                      vtype=grb.GRB.CONTINUOUS,\n",
    "                                      name=f'ReLU{layer_idx}_{neuron_idx}')\n",
    "                if pre_lb >= 0 and pre_ub >= 0:\n",
    "                    # The ReLU is always passing\n",
    "                    gurobi_model.addConstr(v == pre_var)\n",
    "                    lb = pre_lb\n",
    "                    ub = pre_ub\n",
    "                elif pre_lb <= 0 and pre_ub <= 0:\n",
    "                    lb = 0\n",
    "                    ub = 0\n",
    "                    # No need to add an additional constraint that v==0\n",
    "                    # because this will be covered by the bounds we set on\n",
    "                    # the value of v.\n",
    "                else:\n",
    "                    lb = 0\n",
    "                    ub = pre_ub\n",
    "                    gurobi_model.addConstr(v >= pre_var)\n",
    "\n",
    "                    slope = pre_ub / (pre_ub - pre_lb)\n",
    "                    bias = - pre_lb * slope\n",
    "                    gurobi_model.addConstr(v <= slope * pre_var + bias)\n",
    "\n",
    "                new_layer_lb.append(lb)\n",
    "                new_layer_ub.append(ub)\n",
    "                new_layer_gurobi_vars.append(v)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        lower_bounds.append(new_layer_lb)\n",
    "        upper_bounds.append(new_layer_ub)\n",
    "        gurobi_vars.append(new_layer_gurobi_vars)\n",
    "\n",
    "        layer_idx += 1\n",
    "    # Assert that this is as expected a network with a single output\n",
    "    # assert len(gurobi_vars[-1]) == 1, \"Network doesn't have scalar output\"\n",
    "    \n",
    "    #last layer, minimise\n",
    "    v = gurobi_model.addVar(lb=min(lower_bounds[-1]), ub=max(upper_bounds[-1]), obj=0,\n",
    "                                          vtype=grb.GRB.CONTINUOUS,\n",
    "                                          name=f'lay{layer_idx}_min')\n",
    "#     gurobi_model.addConstr(v == min(gurobi_vars[-1]))\n",
    "    gurobi_model.addGenConstrMin(v, gurobi_vars[-1], name= \"minconstr\")\n",
    "    gurobi_model.update()\n",
    "#     print(f'v={v}')\n",
    "    gurobi_model.setObjective(v, grb.GRB.MINIMIZE)\n",
    "    gurobi_model.optimize()\n",
    "    \n",
    "    gurobi_model.update()\n",
    "    gurobi_vars.append([v])\n",
    "   \n",
    "    # We will first setup the appropriate bounds for the elements of the\n",
    "    # input\n",
    "    #is it just to be sure?\n",
    "    for var_idx, inp_var in enumerate(gurobi_vars[0]):\n",
    "        inp_var.lb = domain[0,0,var_idx]\n",
    "        inp_var.ub = domain[1,0,var_idx]\n",
    "\n",
    "    # We will make sure that the objective function is properly set up\n",
    "    gurobi_model.setObjective(gurobi_vars[-1][0], grb.GRB.MINIMIZE)\n",
    "    print(f'gurobi_vars[-1][0].size()={len(gurobi_vars[-1])}')\n",
    "    # We will now compute the requested lower bound\n",
    "    gurobi_model.update()\n",
    "    gurobi_model.optimize()\n",
    "    assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "    print(f'gurobi status {gurobi_model.status}')\n",
    "    print(f'Result={gurobi_vars[-1][0].X}')\n",
    "    print(f'Result={gurobi_vars[-1]}')\n",
    "    print(f'Result -1={gurobi_vars[-2]}')\n",
    "    return gurobi_vars[-1][0].X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784])\n",
      "layer_idx=1\n",
      "Linear\n",
      "layer_idx=2\n",
      "Relu\n",
      "layer_idx=3\n",
      "Linear\n",
      "layer_idx=4\n",
      "Linear\n",
      "gurobi_vars[-1][0].size()=1\n",
      "gurobi status 2\n",
      "Result=6.822462837675309\n",
      "Result=[<gurobi.Var lay5_min (value 6.822462837675309)>]\n",
      "Result -1=[<gurobi.Var lay4_0 (value 10.984329808487798)>, <gurobi.Var lay4_1 (value 15.333439391622132)>, <gurobi.Var lay4_2 (value 8.257162653656147)>, <gurobi.Var lay4_3 (value 6.822462837675309)>, <gurobi.Var lay4_4 (value 14.8620829434579)>, <gurobi.Var lay4_5 (value 10.913033766485475)>, <gurobi.Var lay4_6 (value 21.467261508123308)>, <gurobi.Var lay4_7 (value 10.029630793629961)>, <gurobi.Var lay4_8 (value 9.626551031901219)>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.822462837675309"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lower_bound(domain,verification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
