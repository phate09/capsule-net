{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CodeCell': {'cm_config': {'autoCloseBrackets': False}},\n",
       " 'Cell': {'cm_config': {'lineNumbers': True}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plnn.mini_net import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain(input_tensor, eps_size):\n",
    "    return torch.stack((input_tensor - eps_size, input_tensor + eps_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('save/mini_net.pt'))\n",
    "model.cuda()\n",
    "dataset = MNIST('./data', train=True, download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])),  # load the testing dataset\n",
    "batch_size=10\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=1)#retrieve items 1 at a time\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationNetwork(nn.Module):\n",
    "    def __init__(self, base_network,true_class_index):\n",
    "        super(VerificationNetwork, self).__init__()\n",
    "        self.true_class_index = true_class_index\n",
    "#         self.property_layers=[]\n",
    "#         n_classes = base_network.layers[-1].out_features\n",
    "#         print(f'n_classes={n_classes}')\n",
    "#         for true_class_index in range(n_classes):\n",
    "#             self.property_layers.append( self.attach_property_layers(base_network,true_class_index))\n",
    "        self.property_layer = self.attach_property_layers(base_network,self.true_class_index)\n",
    "        self.layers=base_network.layers+[self.property_layer]\n",
    "        self.out=nn.Sequential(*self.layers)\n",
    "\n",
    "    '''need to  repeat this method for each class so that it describes the distance between the corresponding class \n",
    "    and the closest other class'''\n",
    "    def attach_property_layers(self, model: Net,true_class_index:int):\n",
    "        n_classes = model.layers[-1].out_features\n",
    "        cases = []\n",
    "        for i in range(n_classes):\n",
    "            if i == true_class_index:\n",
    "                continue\n",
    "            case = [0] * n_classes  # list of zeroes\n",
    "            case[true_class_index] = 1  # sets the property to 1\n",
    "            case[i] = -1\n",
    "            cases.append(case)\n",
    "        weights = np.array(cases)\n",
    "#         print(f'weight={weights}')\n",
    "        print(f'weights.size()={weights.shape}')\n",
    "        weightTensor = nn.Linear(in_features=n_classes, out_features=n_classes-1,\n",
    "                                 bias=False)\n",
    "        print(f'initial weightTensor size={weightTensor.weight.size()}')\n",
    "        weightTensor.weight.data = torch.from_numpy(weights).float()\n",
    "        print(f'final weightTensor size={weightTensor.weight.size()}')\n",
    "        return weightTensor\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.base_network(x)\n",
    "        x = self.out(x)\n",
    "        print(x)\n",
    "        print(x.size())\n",
    "        return torch.min(x,dim=1,keepdim=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:torch.Size([10, 1, 28, 28])\n",
      "domain size:torch.Size([2, 10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#get the data and label\n",
    "data, target =next(iter(test_loader))\n",
    "print(f'data size:{data.size()}')\n",
    "# print(data[0])\n",
    "# create the domain\n",
    "domain_raw = generate_domain(data,0.001)\n",
    "data_size=data.size()\n",
    "print(f'domain size:{domain_raw.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "True class=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n",
      "weights.size()=(9, 10)\n",
      "initial weightTensor size=torch.Size([9, 10])\n",
      "final weightTensor size=torch.Size([9, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VerificationNetwork(\n",
       "  (property_layer): Linear(in_features=10, out_features=9, bias=False)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=9, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[-1].out_features)\n",
    "print(f'True class={target}')\n",
    "single_true_class=7\n",
    "verification_model=VerificationNetwork(model,single_true_class)\n",
    "verification_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.1482e+00,  1.7134e+01,  5.5038e+00,  7.1094e+00,  1.8042e+01,\n",
      "          1.2420e+01,  2.3254e+01,  1.0813e+01,  8.8139e+00],\n",
      "        [-1.1975e+01, -5.5618e+00, -1.9245e+01, -1.2707e+01,  1.6361e+00,\n",
      "         -1.0373e+01, -1.0418e+01, -7.5755e+00,  8.4142e+00],\n",
      "        [ 6.5693e+00, -6.6130e+00,  1.7501e-01,  7.4219e-01,  2.9382e+00,\n",
      "          2.0685e+00, -9.5737e-02, -9.0795e-01,  2.9395e+00],\n",
      "        [-1.1448e+01,  1.1901e+01, -3.2770e+00,  4.5974e-01,  1.1652e+01,\n",
      "         -2.9454e+00, -8.2134e-01,  3.3243e+00,  1.1987e+00],\n",
      "        [ 4.2202e+00,  8.4472e+00,  1.6200e+00,  3.3807e+00, -5.7340e+00,\n",
      "          1.7241e+00,  8.5370e-01,  1.0267e+00, -1.3967e+00],\n",
      "        [ 7.9780e+00, -7.1432e+00,  9.6349e-01, -1.7430e-02,  4.4788e+00,\n",
      "          3.9420e+00,  3.1346e+00, -1.2931e+00,  1.5249e+00],\n",
      "        [ 6.1263e+00,  6.1646e+00,  6.1965e+00,  2.9349e+00, -7.8680e+00,\n",
      "         -8.9098e-02,  3.3975e+00, -2.8526e+00, -2.3372e+00],\n",
      "        [ 8.1586e+00,  5.3207e-01, -3.4728e+00, -3.9802e+00, -1.1471e+00,\n",
      "         -6.4116e-01,  6.0744e+00,  2.5119e+00, -6.3930e+00],\n",
      "        [-6.8672e+00, -8.7894e-01, -2.0408e+00,  1.7870e+00, -8.4867e+00,\n",
      "         -9.8742e+00, -1.5552e+01, -5.7456e+00, -2.5390e+00],\n",
      "        [ 9.9643e+00,  8.0697e+00,  9.7241e+00,  5.6611e+00, -9.5353e-01,\n",
      "          8.6314e+00,  9.8434e+00, -2.1285e-01, -4.5808e+00]], device='cuda:0',\n",
      "       grad_fn=<MmBackward>)\n",
      "torch.Size([10, 9])\n",
      "test_out=tensor([[  5.5038],\n",
      "        [-19.2454],\n",
      "        [ -6.6130],\n",
      "        [-11.4482],\n",
      "        [ -5.7340],\n",
      "        [ -7.1432],\n",
      "        [ -7.8680],\n",
      "        [ -6.3930],\n",
      "        [-15.5518],\n",
      "        [ -4.5808]], device='cuda:0', grad_fn=<MinBackward0>)\n",
      "targets=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "test_out=verification_model(data.cuda().view(-1,784))\n",
    "print(f'test_out={test_out}')\n",
    "print(f'targets={target}')\n",
    "# print(f'test_out[0]={test_out[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "test_out=model(data.cuda().view(-1,784))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 784])\n"
     ]
    }
   ],
   "source": [
    "domain=domain_raw.view(2,batch_size,-1)\n",
    "print(domain.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_ub_point, global_ub = net.get_upper_bound(domain)\n",
    "# global_lb = net.get_lower_bound(domain)\n",
    "\n",
    "\n",
    "def get_upper_bound(domain,model):\n",
    "    #we try get_upper_bound\n",
    "    nb_samples = 1024\n",
    "    nb_inp = domain.size()[2:]  #get last dimensions\n",
    "    print(nb_inp)\n",
    "    # Not a great way of sampling but this will be good enough\n",
    "    # We want to get rows that are >= 0\n",
    "    rand_samples_size = [batch_size, nb_samples] + list(nb_inp)\n",
    "    print(rand_samples_size)\n",
    "    rand_samples = torch.zeros(rand_samples_size)\n",
    "    print(rand_samples.size())\n",
    "    # print(rand_samples)\n",
    "    rand_samples.uniform_(0, 1)\n",
    "    # print(rand_samples)\n",
    "    print(rand_samples.size())\n",
    "    domain_lb = domain.select(0, 0).contiguous()\n",
    "    domain_ub = domain.select(0, 1).contiguous()\n",
    "    # print(domain_lb)\n",
    "    print(domain_lb.size())\n",
    "    # print(domain_ub)\n",
    "    print(domain_ub.size())\n",
    "    domain_width = domain_ub - domain_lb\n",
    "    print(domain_width.size())\n",
    "    print(domain_lb.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    print(domain_width.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    domain_lb = domain_lb.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the initial point for the number of examples\n",
    "    print(domain_lb.size())\n",
    "    domain_width = domain_width.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the width for the number of examples\n",
    "    print(domain_width.size())\n",
    "    #those should be the same\n",
    "    print(domain_width.size())\n",
    "    print(rand_samples.size())\n",
    "    inps = domain_lb + domain_width * rand_samples\n",
    "    # print(inps) #each row shuld be different\n",
    "    print(inps.size())\n",
    "    #now flatten the first dimension into the second\n",
    "    flattened_size = [inps.size(0) * inps.size(1)] + list(inps.size()[2:])\n",
    "    print(flattened_size)\n",
    "    #rearrange the tensor so that is consumable by the model\n",
    "    print(data_size)\n",
    "    examples_data_size = [flattened_size[0]] + list(data_size[1:])  #the expected dimension of the example tensor\n",
    "    print(examples_data_size)\n",
    "    var_inps = torch.Tensor(inps).view(examples_data_size)\n",
    "    print(f'var_inps.size()={var_inps.size()}')  #should match data_size\n",
    "    print(inps.size())\n",
    "    # print(inps[0][0])\n",
    "    # print(inps[0][1])\n",
    "    # print(var_inps[0][0])\n",
    "    # print(var_inps[1][0])\n",
    "    outs = model.forward(var_inps.cuda())  #gets the input for the values\n",
    "    print(outs.size())\n",
    "    print(outs[0])  #those two should be very similar but different because they belong to two different random examples\n",
    "    print(outs[1])\n",
    "    print(target.unsqueeze(1))\n",
    "    target_expanded = target.unsqueeze(1).expand(\n",
    "        [batch_size, nb_samples])  #generates nb_samples copies of the target vector, all rows should be the same\n",
    "    print(target_expanded.size())\n",
    "    print(target_expanded)\n",
    "    target_idxs = target_expanded.contiguous().view(\n",
    "        batch_size * nb_samples)  #contains a list of indices that tells which columns out of the 10 classes to pick\n",
    "    print(target_idxs.size())  #the first dimension should match\n",
    "    print(outs.size())\n",
    "    print(outs[target_idxs[0]].size())\n",
    "    outs_true_class = outs.gather(1, target_idxs.cuda().view(-1,\n",
    "                                                             1))  #we choose dimension 1 because it's the one we want to reduce\n",
    "    print(outs_true_class.size())\n",
    "    # print(outs[0])\n",
    "    # print(target_idxs[1])\n",
    "    # print(outs[1][0])#these two should be similar but different because they belong to different examples\n",
    "    # print(outs[0][0])\n",
    "    print(outs_true_class.size())\n",
    "    outs_true_class_resized = outs_true_class.view(batch_size, nb_samples)\n",
    "    print(outs_true_class_resized.size())  #resize outputs so that they each row is a different element of each batch\n",
    "    upper_bound, idx = torch.min(outs_true_class_resized,\n",
    "                                 dim=1)  #this returns the distance of the network output from the given class, it selects the class which is furthest from the current one\n",
    "    print(upper_bound.size())\n",
    "    print(idx.size())\n",
    "    print(idx)\n",
    "    print(upper_bound)\n",
    "    # rearranged_idx=idx.view(list(inps.size()[0:2]))\n",
    "    # print(rearranged_idx.size()) #rearranged idx contains the indexes of the minimum class for each example, for each element of the batch\n",
    "    print(f'idx size {idx.size()}')\n",
    "    print(f'inps size {inps.size()}')\n",
    "    print(idx[0])\n",
    "    # upper_bound = upper_bound[0]\n",
    "    unsqueezed_idx = idx.cuda().view(-1, 1)\n",
    "    print(f'single size {inps[0][unsqueezed_idx[0][0]][:].size()}')\n",
    "    print(f'single size {inps[1][unsqueezed_idx[1][0]][:].size()}')\n",
    "    print(f'single size {inps[2][unsqueezed_idx[2][0]][:].size()}')\n",
    "    ub_point = [inps[x][idx[x]][:].numpy() for x in range(idx.size()[0])]\n",
    "    ub_point = torch.tensor(ub_point)\n",
    "    print(\n",
    "        ub_point)  #ub_point represents the input that amongst all examples returns the minimum response for the appropriate class\n",
    "    print(ub_point.size())\n",
    "    # print(unsqueezed_idx.size())\n",
    "    # ub_point = torch.gather(inps.cuda(),1,unsqueezed_idx.cuda())#todo for some reason it doesn't want to work\n",
    "    # print(ub_point.size())\n",
    "    return ub_point, upper_bound\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test the method\n",
    "get_upper_bound(domain,verification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "input_domain.size()=torch.Size([2, 784])\n"
     ]
    }
   ],
   "source": [
    "print(domain.size()[1])\n",
    "input_domain=domain.select(1,1)#we use a single domain, not ready for parallelisation yet\n",
    "print(f'input_domain.size()={input_domain.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lower_bound(domain,model):\n",
    "    '''\n",
    "    input_domain: Tensor containing in each row the lower and upper bound\n",
    "                  for the corresponding dimension\n",
    "    '''\n",
    "    #now try to do the lower bound\n",
    "    import gurobipy as grb\n",
    "    batch_size=domain.size()[1]\n",
    "    for index in range(batch_size):\n",
    "        input_domain=domain.select(1,index)#we use a single domain, not ready for parallelisation yet\n",
    "        print(f'input_domain.size()={input_domain.size()}')\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        gurobi_vars = []\n",
    "        # These three are nested lists. Each of their elements will itself be a\n",
    "        # list of the neurons after a layer.\n",
    "\n",
    "        gurobi_model = grb.Model()\n",
    "        gurobi_model.setParam('OutputFlag', False)\n",
    "        gurobi_model.setParam('Threads', 1)\n",
    "\n",
    "\n",
    "\n",
    "        ## Do the input layer, which is a special case\n",
    "        inp_lb = []\n",
    "        inp_ub = []\n",
    "        inp_gurobi_vars = []\n",
    "        for dim in range(input_domain.size()[1]):\n",
    "            ub=input_domain[1][dim]#check this value, it can be messed up\n",
    "            lb=input_domain[0][dim]\n",
    "        #     print(f'ub={ub} lb={lb}')\n",
    "            assert ub>lb , \"ub should be greater that lb\"\n",
    "            #     print(f'ub={ub} lb={lb}')\n",
    "            v = gurobi_model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                                  vtype=grb.GRB.CONTINUOUS,\n",
    "                                  name=f'inp_{dim}')\n",
    "            inp_gurobi_vars.append(v)\n",
    "            inp_lb.append(lb)\n",
    "            inp_ub.append(ub)\n",
    "        gurobi_model.update()\n",
    "\n",
    "        lower_bounds.append(inp_lb)\n",
    "        upper_bounds.append(inp_ub)\n",
    "        gurobi_vars.append(inp_gurobi_vars)\n",
    "\n",
    "        # print(lower_bounds[0][0])\n",
    "        # print(upper_bounds[0][0])\n",
    "\n",
    "        # print(model.layers[0])\n",
    "        # print(range(0))\n",
    "\n",
    "        layer_idx = 1\n",
    "        for layer in model.layers:\n",
    "            print(f'layer_idx={layer_idx}')\n",
    "            # layer = model.layers[0]\n",
    "            new_layer_lb = []\n",
    "            new_layer_ub = []\n",
    "            new_layer_gurobi_vars = []\n",
    "            if type(layer) is nn.Linear:\n",
    "                print(f'Linear')\n",
    "                for neuron_idx in range(layer.weight.size(0)):\n",
    "                    if(layer.bias is None):\n",
    "                        ub = 0\n",
    "                        lb = 0\n",
    "                        lin_expr = 0\n",
    "                    else:\n",
    "                        ub = layer.bias.data[neuron_idx]\n",
    "                        lb = layer.bias.data[neuron_idx]\n",
    "                        lin_expr = layer.bias.data[neuron_idx].item() #adds the bias to the linear expression\n",
    "                #     print(f'bias_ub={ub} bias_lb={lb}')\n",
    "\n",
    "                    for prev_neuron_idx in range(layer.weight.size(1)):\n",
    "                        coeff = layer.weight.data[neuron_idx, prev_neuron_idx]#picks the weight between the two neurons\n",
    "                #         print(f'coeff={coeff} upper={coeff*upper_bounds[-1][prev_neuron_idx]} lower={coeff*lower_bounds[-1][prev_neuron_idx]}')\n",
    "        #                 assert coeff*lower_bounds[-1][prev_neuron_idx]!=coeff*upper_bounds[-1][prev_neuron_idx], f\"coeff={coeff} upper={coeff*upper_bounds[-1][prev_neuron_idx]} lower={coeff*lower_bounds[-1][prev_neuron_idx]}\"\n",
    "                        if coeff>=0:\n",
    "                            ub = ub+ coeff*upper_bounds[-1][prev_neuron_idx]#multiplies the ub\n",
    "                            lb = lb+ coeff*lower_bounds[-1][prev_neuron_idx]#multiplies the lb\n",
    "                        else: #inverted\n",
    "                            ub = ub+ coeff*lower_bounds[-1][prev_neuron_idx]#multiplies the ub\n",
    "                            lb = lb+ coeff*upper_bounds[-1][prev_neuron_idx]#multiplies the lb\n",
    "                #         print(f'ub={ub} lb={lb}')\n",
    "    #                     assert ub!=lb\n",
    "                        lin_expr = lin_expr+ coeff.item() * gurobi_vars[-1][prev_neuron_idx]#multiplies the unknown by the coefficient\n",
    "                #         print(lin_expr)\n",
    "                    v = gurobi_model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                                              vtype=grb.GRB.CONTINUOUS,\n",
    "                                              name=f'lay{layer_idx}_{neuron_idx}')\n",
    "                    gurobi_model.addConstr(v == lin_expr)\n",
    "                    gurobi_model.update()\n",
    "                #     print(f'v={v}')\n",
    "                    gurobi_model.setObjective(v, grb.GRB.MINIMIZE)\n",
    "                    gurobi_model.optimize()\n",
    "                #          print(f'gurobi status {gurobi_model.status}')\n",
    "                    assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "                    # We have computed a lower bound\n",
    "                    lb = v.X\n",
    "                    v.lb = lb\n",
    "\n",
    "                    # Let's now compute an upper bound\n",
    "                    gurobi_model.setObjective(v, grb.GRB.MAXIMIZE)\n",
    "                    gurobi_model.update()\n",
    "                    gurobi_model.reset()\n",
    "                    gurobi_model.optimize()\n",
    "                    assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "                    ub = v.X\n",
    "                    v.ub = ub\n",
    "\n",
    "                    new_layer_lb.append(lb)\n",
    "                    new_layer_ub.append(ub)\n",
    "                    new_layer_gurobi_vars.append(v)\n",
    "            elif type(layer) == nn.ReLU:\n",
    "                print('Relu')\n",
    "                for neuron_idx, pre_var in enumerate(gurobi_vars[-1]):\n",
    "                    pre_lb = lower_bounds[-1][neuron_idx]\n",
    "                    pre_ub = upper_bounds[-1][neuron_idx]\n",
    "\n",
    "                    v = gurobi_model.addVar(lb=max(0, pre_lb),\n",
    "                                          ub=max(0, pre_ub),\n",
    "                                          obj=0,\n",
    "                                          vtype=grb.GRB.CONTINUOUS,\n",
    "                                          name=f'ReLU{layer_idx}_{neuron_idx}')\n",
    "                    if pre_lb >= 0 and pre_ub >= 0:\n",
    "                        # The ReLU is always passing\n",
    "                        gurobi_model.addConstr(v == pre_var)\n",
    "                        lb = pre_lb\n",
    "                        ub = pre_ub\n",
    "                    elif pre_lb <= 0 and pre_ub <= 0:\n",
    "                        lb = 0\n",
    "                        ub = 0\n",
    "                        # No need to add an additional constraint that v==0\n",
    "                        # because this will be covered by the bounds we set on\n",
    "                        # the value of v.\n",
    "                    else:\n",
    "                        lb = 0\n",
    "                        ub = pre_ub\n",
    "                        gurobi_model.addConstr(v >= pre_var)\n",
    "\n",
    "                        slope = pre_ub / (pre_ub - pre_lb)\n",
    "                        bias = - pre_lb * slope\n",
    "                        gurobi_model.addConstr(v <= slope * pre_var + bias)\n",
    "\n",
    "                    new_layer_lb.append(lb)\n",
    "                    new_layer_ub.append(ub)\n",
    "                    new_layer_gurobi_vars.append(v)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            lower_bounds.append(new_layer_lb)\n",
    "            upper_bounds.append(new_layer_ub)\n",
    "            gurobi_vars.append(new_layer_gurobi_vars)\n",
    "\n",
    "            layer_idx += 1\n",
    "        # Assert that this is as expected a network with a single output\n",
    "        # assert len(gurobi_vars[-1]) == 1, \"Network doesn't have scalar output\"\n",
    "\n",
    "        #last layer, minimise\n",
    "        v = gurobi_model.addVar(lb=min(lower_bounds[-1]), ub=max(upper_bounds[-1]), obj=0,\n",
    "                                              vtype=grb.GRB.CONTINUOUS,\n",
    "                                              name=f'lay{layer_idx}_min')\n",
    "    #     gurobi_model.addConstr(v == min(gurobi_vars[-1]))\n",
    "        gurobi_model.addGenConstrMin(v, gurobi_vars[-1], name= \"minconstr\")\n",
    "        gurobi_model.update()\n",
    "    #     print(f'v={v}')\n",
    "        gurobi_model.setObjective(v, grb.GRB.MINIMIZE)\n",
    "        gurobi_model.optimize()\n",
    "\n",
    "        gurobi_model.update()\n",
    "        gurobi_vars.append([v])\n",
    "\n",
    "        # We will first setup the appropriate bounds for the elements of the\n",
    "        # input\n",
    "        #is it just to be sure?\n",
    "        for var_idx, inp_var in enumerate(gurobi_vars[0]):\n",
    "            inp_var.lb = domain[0,0,var_idx]\n",
    "            inp_var.ub = domain[1,0,var_idx]\n",
    "\n",
    "        # We will make sure that the objective function is properly set up\n",
    "        gurobi_model.setObjective(gurobi_vars[-1][0], grb.GRB.MINIMIZE)\n",
    "        print(f'gurobi_vars[-1][0].size()={len(gurobi_vars[-1])}')\n",
    "        # We will now compute the requested lower bound\n",
    "        gurobi_model.update()\n",
    "        gurobi_model.optimize()\n",
    "        assert gurobi_model.status == 2, \"LP wasn't optimally solved\"\n",
    "        print(f'gurobi status {gurobi_model.status}')\n",
    "        print(f'Result={gurobi_vars[-1][0].X}')\n",
    "        print(f'Result={gurobi_vars[-1]}')\n",
    "        print(f'Result -1={gurobi_vars[-2]}')\n",
    "        return gurobi_vars[-1][0].X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test the lower bound\n",
    "get_lower_bound(domain,verification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
