{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CodeCell': {'cm_config': {'autoCloseBrackets': False}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plnn.mnist_basic import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain(input_tensor, eps_size):\n",
    "    return torch.stack((input_tensor - eps_size, input_tensor + eps_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('save/mnist_cnn.pt'))\n",
    "model.cuda()\n",
    "dataset = MNIST('./data', train=True, download=True,\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])),  # load the testing dataset\n",
    "batch_size=10\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=1)#retrieve items 1 at a time\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationNetwork(nn.Module):\n",
    "    def __init__(self, in_features, out_features, base_network, out_function, true_class_index):\n",
    "        super(VerificationNetwork, self).__init__()\n",
    "        self.true_class_index = true_class_index\n",
    "        self.out_function = out_function\n",
    "        self.base_network = base_network\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "        self.property_layer = self.attach_property_layers(self.base_network, self.true_class_index)\n",
    "\n",
    "    '''need to  repeat this method for each class so that it describes the distance between the corresponding class \n",
    "    and the closest other class'''\n",
    "    def attach_property_layers(self, model: Net, true_class_index: int):\n",
    "        n_classes = model.fc2.out_features\n",
    "        cases = []\n",
    "        for i in range(n_classes):\n",
    "            if i == true_class_index:\n",
    "                continue\n",
    "            case = [0] * n_classes  # list of zeroes\n",
    "            case[true_class_index] = 1  # sets the property to 1\n",
    "            case[i] = -1\n",
    "            cases.append(case)\n",
    "        weights = np.array(cases)\n",
    "        weightTensor = nn.Linear(in_features=n_classes, out_features=n_classes,\n",
    "                                 bias=False)\n",
    "        weightTensor.weight.data = torch.from_numpy(weights).float()\n",
    "        return weightTensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_network(x)\n",
    "        x = self.property_layer(x)\n",
    "        print(x)\n",
    "        print(x.size())\n",
    "        return torch.min(x,dim=1,keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:torch.Size([10, 1, 28, 28])\n",
      "domain size:torch.Size([2, 10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#get the data and label\n",
    "data, target =next(iter(test_loader))\n",
    "print(f'data size:{data.size()}')\n",
    "# print(data[0])\n",
    "# create the domain\n",
    "domain_raw = generate_domain(data,0.001)\n",
    "data_size=data.size()\n",
    "print(f'domain size:{domain_raw.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "test_out=model(data.cuda())\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 784])\n"
     ]
    }
   ],
   "source": [
    "domain=domain_raw.view(2,batch_size,-1)\n",
    "print(domain.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_ub_point, global_ub = net.get_upper_bound(domain)\n",
    "# global_lb = net.get_lower_bound(domain)\n",
    "\n",
    "\n",
    "def get_upper_bound(domain):\n",
    "    #we try get_upper_bound\n",
    "    nb_samples = 1024\n",
    "    nb_inp = domain.size()[2:]  #get last dimensions\n",
    "    print(nb_inp)\n",
    "    # Not a great way of sampling but this will be good enough\n",
    "    # We want to get rows that are >= 0\n",
    "    rand_samples_size = [batch_size, nb_samples] + list(nb_inp)\n",
    "    print(rand_samples_size)\n",
    "    rand_samples = torch.zeros(rand_samples_size)\n",
    "    print(rand_samples.size())\n",
    "    # print(rand_samples)\n",
    "    rand_samples.uniform_(0, 1)\n",
    "    # print(rand_samples)\n",
    "    print(rand_samples.size())\n",
    "    domain_lb = domain.select(0, 0).contiguous()\n",
    "    domain_ub = domain.select(0, 1).contiguous()\n",
    "    # print(domain_lb)\n",
    "    print(domain_lb.size())\n",
    "    # print(domain_ub)\n",
    "    print(domain_ub.size())\n",
    "    domain_width = domain_ub - domain_lb\n",
    "    print(domain_width.size())\n",
    "    print(domain_lb.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    print(domain_width.view([batch_size, 1] + list(nb_inp)).size())\n",
    "    domain_lb = domain_lb.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the initial point for the number of examples\n",
    "    print(domain_lb.size())\n",
    "    domain_width = domain_width.view([batch_size, 1] + list(nb_inp)).expand(\n",
    "        [batch_size, nb_samples] + list(nb_inp))  #expand the width for the number of examples\n",
    "    print(domain_width.size())\n",
    "    #those should be the same\n",
    "    print(domain_width.size())\n",
    "    print(rand_samples.size())\n",
    "    inps = domain_lb + domain_width * rand_samples\n",
    "    # print(inps) #each row shuld be different\n",
    "    print(inps.size())\n",
    "    #now flatten the first dimension into the second\n",
    "    flattened_size = [inps.size(0) * inps.size(1)] + list(inps.size()[2:])\n",
    "    print(flattened_size)\n",
    "    #rearrange the tensor so that is consumable by the model\n",
    "    print(data_size)\n",
    "    examples_data_size = [flattened_size[0]] + list(data_size[1:])  #the expected dimension of the example tensor\n",
    "    print(examples_data_size)\n",
    "    var_inps = torch.Tensor(inps).view(examples_data_size)\n",
    "    print(var_inps.size())  #should match data_size\n",
    "    print(inps.size())\n",
    "    # print(inps[0][0])\n",
    "    # print(inps[0][1])\n",
    "    # print(var_inps[0][0])\n",
    "    # print(var_inps[1][0])\n",
    "    outs = model.forward(var_inps.cuda())  #gets the input for the values\n",
    "    print(outs.size())\n",
    "    print(outs[0])  #those two should be very similar but different because they belong to two different random examples\n",
    "    print(outs[1])\n",
    "    print(target.unsqueeze(1))\n",
    "    target_expanded = target.unsqueeze(1).expand(\n",
    "        [batch_size, nb_samples])  #generates nb_samples copies of the target vector, all rows should be the same\n",
    "    print(target_expanded.size())\n",
    "    print(target_expanded)\n",
    "    target_idxs = target_expanded.contiguous().view(\n",
    "        batch_size * nb_samples)  #contains a list of indices that tells which columns out of the 10 classes to pick\n",
    "    print(target_idxs.size())  #the first dimension should match\n",
    "    print(outs.size())\n",
    "    print(outs[target_idxs[0]].size())\n",
    "    outs_true_class = outs.gather(1, target_idxs.cuda().view(-1,\n",
    "                                                             1))  #we choose dimension 1 because it's the one we want to reduce\n",
    "    print(outs_true_class.size())\n",
    "    # print(outs[0])\n",
    "    # print(target_idxs[1])\n",
    "    # print(outs[1][0])#these two should be similar but different because they belong to different examples\n",
    "    # print(outs[0][0])\n",
    "    print(outs_true_class.size())\n",
    "    outs_true_class_resized = outs_true_class.view(batch_size, nb_samples)\n",
    "    print(outs_true_class_resized.size())  #resize outputs so that they each row is a different element of each batch\n",
    "    upper_bound, idx = torch.min(outs_true_class_resized,\n",
    "                                 dim=1)  #this returns the distance of the network output from the given class, it selects the class which is furthest from the current one\n",
    "    print(upper_bound.size())\n",
    "    print(idx.size())\n",
    "    print(idx)\n",
    "    print(upper_bound)\n",
    "    # rearranged_idx=idx.view(list(inps.size()[0:2]))\n",
    "    # print(rearranged_idx.size()) #rearranged idx contains the indexes of the minimum class for each example, for each element of the batch\n",
    "    print(f'idx size {idx.size()}')\n",
    "    print(f'inps size {inps.size()}')\n",
    "    print(idx[0])\n",
    "    # upper_bound = upper_bound[0]\n",
    "    unsqueezed_idx = idx.cuda().view(-1, 1)\n",
    "    print(f'single size {inps[0][unsqueezed_idx[0][0]][:].size()}')\n",
    "    print(f'single size {inps[1][unsqueezed_idx[1][0]][:].size()}')\n",
    "    print(f'single size {inps[2][unsqueezed_idx[2][0]][:].size()}')\n",
    "    ub_point = [inps[x][idx[x]][:].numpy() for x in range(idx.size()[0])]\n",
    "    ub_point = torch.tensor(ub_point)\n",
    "    print(\n",
    "        ub_point)  #ub_point represents the input that amongst all examples returns the minimum response for the appropriate class\n",
    "    print(ub_point.size())\n",
    "    # print(unsqueezed_idx.size())\n",
    "    # ub_point = torch.gather(inps.cuda(),1,unsqueezed_idx.cuda())#todo for some reason it doesn't want to work\n",
    "    # print(ub_point.size())\n",
    "    return ub_point, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the method\n",
    "get_upper_bound(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try to do the lower bound\n",
    "import gurobipy as grb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input_domain: Tensor containing in each row the lower and upper bound\n",
    "              for the corresponding dimension\n",
    "'''\n",
    "lower_bounds = []\n",
    "upper_bounds = []\n",
    "gurobi_vars = []\n",
    "# These three are nested lists. Each of their elements will itself be a\n",
    "# list of the neurons after a layer.\n",
    "\n",
    "gurobi_model = grb.Model()\n",
    "gurobi_model.setParam('OutputFlag', False)\n",
    "gurobi_model.setParam('Threads', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784])\n"
     ]
    }
   ],
   "source": [
    "input_domain=domain.select(1,0)#we use a single domain, not ready for parallelisation yet\n",
    "print(input_domain.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the input layer, which is a special case\n",
    "inp_lb = []\n",
    "inp_ub = []\n",
    "inp_gurobi_vars = []\n",
    "for dim in range(input_domain.size()[1]):\n",
    "    ub=input_domain[0][dim]\n",
    "    lb=input_domain[1][dim]\n",
    "    v = gurobi_model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                          vtype=grb.GRB.CONTINUOUS,\n",
    "                          name=f'inp_{dim}')\n",
    "    inp_gurobi_vars.append(v)\n",
    "    inp_lb.append(lb)\n",
    "    inp_ub.append(ub)\n",
    "gurobi_model.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds.append(inp_lb)\n",
    "upper_bounds.append(inp_ub)\n",
    "gurobi_vars.append(inp_gurobi_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now i need to list each layer in the model\n",
    "layers = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Do the other layers, computing for each of the neuron, its upper\n",
    "## bound and lower bound\n",
    "layer_idx = 1\n",
    "for layer in self.layers:\n",
    "    new_layer_lb = []\n",
    "    new_layer_ub = []\n",
    "    new_layer_gurobi_vars = []\n",
    "    if type(layer) is nn.Linear:\n",
    "        for neuron_idx in range(layer.weight.size(0)):\n",
    "            ub = layer.bias.data[neuron_idx]\n",
    "            lb = layer.bias.data[neuron_idx]\n",
    "            lin_expr = layer.bias.data[neuron_idx]\n",
    "            for prev_neuron_idx in range(layer.weight.size(1)):\n",
    "                coeff = layer.weight.data[neuron_idx, prev_neuron_idx]\n",
    "                if coeff >= 0:\n",
    "                    ub += coeff*self.upper_bounds[-1][prev_neuron_idx]\n",
    "                    lb += coeff*self.lower_bounds[-1][prev_neuron_idx]\n",
    "                else:\n",
    "                    ub += coeff*self.lower_bounds[-1][prev_neuron_idx]\n",
    "                    lb += coeff*self.upper_bounds[-1][prev_neuron_idx]\n",
    "                lin_expr += coeff * self.gurobi_vars[-1][prev_neuron_idx]\n",
    "            v = self.model.addVar(lb=lb, ub=ub, obj=0,\n",
    "                                  vtype=grb.GRB.CONTINUOUS,\n",
    "                                  name=f'lay{layer_idx}_{neuron_idx}')\n",
    "            self.model.addConstr(v == lin_expr)\n",
    "            self.model.update()\n",
    "\n",
    "            self.model.setObjective(v, grb.GRB.MINIMIZE)\n",
    "            self.model.optimize()\n",
    "            assert self.model.status == 2, \"LP wasn't optimally solved\"\n",
    "            # We have computed a lower bound\n",
    "            lb = v.X\n",
    "            v.lb = lb\n",
    "\n",
    "            # Let's now compute an upper bound\n",
    "            self.model.setObjective(v, grb.GRB.MAXIMIZE)\n",
    "            self.model.update()\n",
    "            self.model.reset()\n",
    "            self.model.optimize()\n",
    "            assert self.model.status == 2, \"LP wasn't optimally solved\"\n",
    "            ub = v.X\n",
    "            v.ub = ub\n",
    "\n",
    "            new_layer_lb.append(lb)\n",
    "            new_layer_ub.append(ub)\n",
    "            new_layer_gurobi_vars.append(v)\n",
    "    elif type(layer) == nn.ReLU:\n",
    "        for neuron_idx, pre_var in enumerate(self.gurobi_vars[-1]):\n",
    "            pre_lb = self.lower_bounds[-1][neuron_idx]\n",
    "            pre_ub = self.upper_bounds[-1][neuron_idx]\n",
    "\n",
    "            v = self.model.addVar(lb=max(0, pre_lb),\n",
    "                                  ub=max(0, pre_ub),\n",
    "                                  obj=0,\n",
    "                                  vtype=grb.GRB.CONTINUOUS,\n",
    "                                  name=f'ReLU{layer_idx}_{neuron_idx}')\n",
    "            if pre_lb >= 0 and pre_ub >= 0:\n",
    "                # The ReLU is always passing\n",
    "                self.model.addConstr(v == pre_var)\n",
    "                lb = pre_lb\n",
    "                ub = pre_ub\n",
    "            elif pre_lb <= 0 and pre_ub <= 0:\n",
    "                lb = 0\n",
    "                ub = 0\n",
    "                # No need to add an additional constraint that v==0\n",
    "                # because this will be covered by the bounds we set on\n",
    "                # the value of v.\n",
    "            else:\n",
    "                lb = 0\n",
    "                ub = pre_ub\n",
    "                self.model.addConstr(v >= pre_var)\n",
    "\n",
    "                slope = pre_ub / (pre_ub - pre_lb)\n",
    "                bias = - pre_lb * slope\n",
    "                self.model.addConstr(v <= slope * pre_var + bias)\n",
    "\n",
    "            new_layer_lb.append(lb)\n",
    "            new_layer_ub.append(ub)\n",
    "            new_layer_gurobi_vars.append(v)\n",
    "    elif type(layer) == nn.MaxPool1d:\n",
    "        assert layer.padding == 0, \"Non supported Maxpool option\"\n",
    "        assert layer.dilation == 1, \"Non supported MaxPool option\"\n",
    "        nb_pre = len(self.gurobi_vars[-1])\n",
    "        window_size = layer.kernel_size\n",
    "        stride = layer.stride\n",
    "\n",
    "        pre_start_idx = 0\n",
    "        pre_window_end = pre_start_idx + window_size\n",
    "\n",
    "        while pre_window_end <= nb_pre:\n",
    "            lb = max(self.lower_bounds[-1][pre_start_idx:pre_window_end])\n",
    "            ub = max(self.upper_bounds[-1][pre_start_idx:pre_window_end])\n",
    "\n",
    "            neuron_idx = pre_start_idx // stride\n",
    "\n",
    "            v = self.model.addVar(lb=lb, ub=ub, obj=0, vtype=grb.GRB.CONTINUOUS,\n",
    "                                  name=f'Maxpool{layer_idx}_{neuron_idx}')\n",
    "            all_pre_var = 0\n",
    "            for pre_var in self.gurobi_vars[-1][pre_start_idx:pre_window_end]:\n",
    "                self.model.addConstr(v >= pre_var)\n",
    "                all_pre_var += pre_var\n",
    "            all_lb = sum(self.lower_bounds[-1][pre_start_idx:pre_window_end])\n",
    "            max_pre_lb = lb\n",
    "            self.model.addConstr(all_pre_var >= v + all_lb - max_pre_lb)\n",
    "\n",
    "            pre_start_idx += stride\n",
    "            pre_window_end = pre_start_idx + window_size\n",
    "\n",
    "            new_layer_lb.append(lb)\n",
    "            new_layer_ub.append(ub)\n",
    "            new_layer_gurobi_vars.append(v)\n",
    "    elif type(layer) == View:\n",
    "        continue\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    self.lower_bounds.append(new_layer_lb)\n",
    "    self.upper_bounds.append(new_layer_ub)\n",
    "    self.gurobi_vars.append(new_layer_gurobi_vars)\n",
    "\n",
    "    layer_idx += 1\n",
    "\n",
    "# Assert that this is as expected a network with a single output\n",
    "assert len(self.gurobi_vars[-1]) == 1, \"Network doesn't have scalar output\"\n",
    "\n",
    "self.model.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.size())\n",
    "true_class=torch.argmax(model(data.cuda()),dim=1)\n",
    "print(true_class.size())\n",
    "print(true_class)\n",
    "single_true_class=true_class[0].item()\n",
    "print(single_true_class)\n",
    "verification_model=VerificationNetwork(28,10,model,model.forward,single_true_class)\n",
    "verification_model.cuda()\n",
    "\n",
    "result=verification_model.forward(data.cuda())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
